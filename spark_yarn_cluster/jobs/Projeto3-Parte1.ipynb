{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63182591",
   "metadata": {},
   "source": [
    "<!-- Projeto Desenvolvido na Data Science Academy - www.datascienceacademy.com.br -->\n",
    "# <font color='blue'>Data Science Academy</font>\n",
    "## <font color='blue'>PySpark e Apache Kafka Para Processamento de Dados em Batch e Streaming</font>\n",
    "## <font color='blue'>Projeto 3</font>\n",
    "### <font color='blue'>Pipeline de Limpeza e Transformação Para Aplicações de IA com PySpark SQL</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c6d0b",
   "metadata": {},
   "source": [
    "## Pacotes Python Usados no Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181cb9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.ml.evaluation as evals\n",
    "import pyspark.ml.tuning as tune\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.feature import  VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e159cbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633afd61",
   "metadata": {},
   "source": [
    "## Criando a Sessão Spark e Definindo o Nível de Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d44a0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/29 22:32:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/29 22:32:55 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# Cria a sessão\n",
    "spark = SparkSession.builder.appName('Projeto3-Exp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8982587-b929-41bd-96aa-840580ffede5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/29 22:33:33 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Cria a sessão e especifica parâmetros do cluster\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Projeto3-Exp') \\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.cores', '2') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48c3202f-f2f8-4a53-85ae-437886ac0375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/20 16:49:57 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Cria a sessão Spark com YARN como gerenciador de recursos e especifica parâmetros do cluster\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Projeto3-Exp') \\\n",
    "    .master('yarn') \\\n",
    "    .config('spark.submit.deployMode', 'client') \\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.memory', '1g') \\\n",
    "    .config('spark.executor.cores', '2') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b9114-aa7e-43c3-94b6-9e8f4b7b48ea",
   "metadata": {},
   "source": [
    "**SparkSession.builder \\:** Inicia a construção de uma nova sessão Spark. A SparkSession é a entrada principal para a programação com Spark SQL e fornece uma interface unificada para configurar a aplicação.\n",
    "\n",
    "**.appName('Projeto3-Exp') \\:** Define o nome da aplicação, que será exibido na interface do YARN e do Spark UI. Isso ajuda a identificar a aplicação no cluster.\n",
    "\n",
    "**.master('yarn') \\:** Define o gerenciador de recursos do cluster. Ao especificar 'yarn', você informa ao Spark que deve usar o YARN para gerenciar os recursos da aplicação (como memória e núcleos de CPU).\n",
    "\n",
    "**.config('spark.submit.deployMode', 'client') \\:** Define o modo de execução do driver:\n",
    "\n",
    "- 'client': O driver é executado na máquina local onde o código foi iniciado. Isso é comum em testes ou quando você precisa de interação direta com o driver.\n",
    "\n",
    "- 'cluster': O driver é executado em um dos nós do cluster gerenciado pelo YARN, sendo mais adequado para produção, pois mantém a aplicação independente da máquina de origem. ESSE MODO NÃO FUNCIONA VIA JUPYTER NOTEBOOK.\n",
    "\n",
    "**.config('spark.driver.memory', '4g') \\:** Define a quantidade de memória alocada para o processo do driver (4 GB neste caso). O driver é responsável por gerenciar a execução da aplicação.\n",
    "\n",
    "**.config('spark.executor.memory', '1g') \\:** Define a quantidade de memória alocada para cada executor (1 GB neste caso). Os executores são os processos que executam as tarefas distribuídas pelo cluster.\n",
    "\n",
    "**.config('spark.executor.cores', '2') \\:** Define o número de núcleos de CPU alocados para cada executor (2 núcleos por executor). Isso influencia o paralelismo do processamento das tarefas.\n",
    "\n",
    "**.getOrCreate()**: Finaliza a construção da sessão Spark e retorna a sessão criada. Se uma sessão com as mesmas configurações já estiver ativa, ela será reutilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9055d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o nível de log\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665da3f6",
   "metadata": {},
   "source": [
    "<!-- Projeto Desenvolvido na Data Science Academy - www.datascienceacademy.com.br -->\n",
    "## Carregando os Datasets a Partir do HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb722bd-73a0-46d8-9f48-5a1ea7c64d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 0) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Carrega o arquivo 1\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_dsa_aeroportos \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/opt/spark/data/dataset1.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Carrega o arquivo 1\n",
    "df_dsa_aeroportos = spark.read.csv(\"/opt/spark/data/dataset1.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc07ddd4-7de6-4891-a9f8-916e62262b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_dsa_aeroportos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe512742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+----------+------------+----+---+---+\n",
      "|faa|name                          |lat       |lon         |alt |tz |dst|\n",
      "+---+------------------------------+----------+------------+----+---+---+\n",
      "|04G|Lansdowne Airport             |41.1304722|-80.6195833 |1044|-5 |A  |\n",
      "|06A|Moton Field Municipal Airport |32.4605722|-85.6800278 |264 |-5 |A  |\n",
      "|06C|Schaumburg Regional           |41.9893408|-88.1012428 |801 |-6 |A  |\n",
      "|06N|Randall Airport               |41.431912 |-74.3915611 |523 |-5 |A  |\n",
      "|09J|Jekyll Island Airport         |31.0744722|-81.4277778 |11  |-4 |A  |\n",
      "|0A9|Elizabethton Municipal Airport|36.3712222|-82.1734167 |1593|-4 |A  |\n",
      "|0G6|Williams County Airport       |41.4673056|-84.5067778 |730 |-5 |A  |\n",
      "|0G7|Finger Lakes Regional Airport |42.8835647|-76.7812318 |492 |-5 |A  |\n",
      "|0P2|Shoestring Aviation Airfield  |39.7948244|-76.6471914 |1000|-5 |U  |\n",
      "|0S9|Jefferson County Intl         |48.0538086|-122.8106436|108 |-8 |A  |\n",
      "+---+------------------------------+----------+------------+----+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dsa_aeroportos.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98a99882-5588-4e10-9af4-68a74b29466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o arquivo 2\n",
    "df_dsa_voos = spark.read.csv(\"/opt/spark/data/dataset2.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d6b6e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
      "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
      "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
      "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
      "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
      "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|\n",
      "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|\n",
      "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|\n",
      "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|\n",
      "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dsa_voos.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64508433-ef3f-45a0-9395-bdac80c39a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o arquivo 3\n",
    "df_dsa_aeronaves = spark.read.csv(\"/opt/spark/data/dataset3.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56bbb57d-335c-472d-97b7-b9886e8e8338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----------------------+----------------+--------+-------+-----+-----+---------+\n",
      "|tailnum|year|type                   |manufacturer    |model   |engines|seats|speed|engine   |\n",
      "+-------+----+-----------------------+----------------+--------+-------+-----+-----+---------+\n",
      "|N102UW |1998|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N103US |1999|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N104UW |1999|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N105UW |1999|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N107US |1999|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N108UW |1999|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N109UW |1999|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N110UW |1999|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N111US |1999|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N11206 |2000|Fixed wing multi engine|BOEING          |737-824 |2      |149  |NA   |Turbo-fan|\n",
      "|N112US |1999|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N113UW |1999|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N114UW |1999|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N117UW |2000|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N118US |2000|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N119US |2000|Fixed wing multi engine|AIRBUS INDUSTRIE|A320-214|2      |182  |NA   |Turbo-fan|\n",
      "|N1200K |1998|Fixed wing multi engine|BOEING          |767-332 |2      |330  |NA   |Turbo-fan|\n",
      "|N1201P |1998|Fixed wing multi engine|BOEING          |767-332 |2      |330  |NA   |Turbo-fan|\n",
      "|N12114 |1995|Fixed wing multi engine|BOEING          |757-224 |2      |178  |NA   |Turbo-jet|\n",
      "|N121DE |1987|Fixed wing multi engine|BOEING          |767-332 |2      |330  |NA   |Turbo-fan|\n",
      "+-------+----+-----------------------+----------------+--------+-------+-----+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dsa_aeronaves.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0e416-d75e-4798-a44a-0e967227c3a2",
   "metadata": {},
   "source": [
    "Vamos converter esses dados para o formato:\n",
    "\n",
    "- Dados de entrada --> ['month', 'air_time', 'carr_fact', 'dest_fact', 'plane_age'] como o vetor features.\n",
    "- Dados de saída --> ['is_late'] com o nome label.\n",
    "\n",
    "E então usaremos os dados nesse formato para treinar e avaliar dois modelos de Machine Learning. Escolheremos o melhor modelo e então criaremos o job de automação do processo de treinamento no cluster Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3256da71-37f8-4784-a194-23b9972c361f",
   "metadata": {},
   "source": [
    "## Continuaremos no Próximo Capítulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacde2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6f357",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
